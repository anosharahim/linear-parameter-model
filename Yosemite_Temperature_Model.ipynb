{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9d7b8b",
   "metadata": {},
   "source": [
    "# Yosemite Village yearly weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b553f35",
   "metadata": {},
   "source": [
    "## Part 0 - Data Preprocessing \n",
    "Temperature is cyclical, not only on a 24 hour basis but also on a yearly basis. Convert the dataset into a richer format whereby the day of the year is also captured. For example the time “20150212 1605”, can be converted into (43, 965) because the 12th of February is the 43rd day of the year, and 16:05 is the 965th minute of the day.\n",
    "\n",
    "This data covers 6 years, so split the data into a training set of the first 5 years, and a testing set of the 6th year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b152beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76c9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2011, 2017) \n",
    "files = ['CRNS0101-05-%d-CA_Yosemite_Village_12_W.txt' % y for y in years]\n",
    "usecols = [1, 2, 8] #[UTC_time, UTC date, Temperature]\n",
    "tr = [np.loadtxt(f, usecols=usecols) for f in files]\n",
    "ts = np.loadtxt('CRNS0101-05-2016-CA_Yosemite_Village_12_W.txt', usecols=usecols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9051699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set shape == (105408, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[645. , 339. ,   4.2],\n",
       "       [370. ,   7. ,  -4.3],\n",
       "       [475. ,   0. ,  -4.3],\n",
       "       ...,\n",
       "       [825. , 364. ,   7.4],\n",
       "       [845. , 153. ,  17. ],\n",
       "       [650. , 260. ,  16.3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TEST SET ## \n",
    "\n",
    "#convert dates from YYYYMMDD format to days since start of year\n",
    "ts_year = '20160101'\n",
    "ts_year_start = datetime.strptime(ts_year, '%Y%m%d') #get year start from string\n",
    "ts_date=pd.to_datetime(ts[:,0], format='%Y%m%d') #convert all floats to datetime \n",
    "ts_date.to_numpy()\n",
    "ts_days = ts_date - ts_year_start #compute days since start of year\n",
    "ts_days = ts_days / np.timedelta64(1, 'D') #convert time deltas to integers\n",
    "ts_days=ts_days.to_numpy() #convert back to numpy from pandas dataframe\n",
    "\n",
    "#convert time of day from HHMM to minutes since start of day \n",
    "hour_mins = np.divmod(ts[:,1],np.full((105408,), 100)) #creates two rows for hour and minutes each\n",
    "ts_minutes = (hour_mins[0]*60)+hour_mins[1] #add them together to get new array of time \n",
    "ts_minutes = ts_minutes.astype(int) #convert timestamps to integers\n",
    "\n",
    "#merge all columns\n",
    "test = np.column_stack((ts_minutes, ts_days,ts[:,2]))\n",
    "\n",
    " #an array of minutes, days, and temp columns\n",
    "np.random.shuffle(test)\n",
    "print('test set shape ==', test.shape)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fd4f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set shape == (525888, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 720. ,  331. ,    7.6],\n",
       "       [ 730. ,  311. ,    4.8],\n",
       "       [1130. ,  102. ,   12.5],\n",
       "       ...,\n",
       "       [ 515. ,  361. ,   -6. ],\n",
       "       [1005. ,  341. ,   -8.5],\n",
       "       [ 695. ,  168. ,   16. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## TRAIN SET ## \n",
    "\n",
    "tr_years = ['20110101','20120101','20130101','20140101','20150101']\n",
    "train_dates = [] #list for arrays of days for each year \n",
    "train_minutes = [] #list for array of day minutes for each year\n",
    "\n",
    "#convert date \n",
    "for i in range(len(tr_years)):\n",
    "\n",
    "    tr_year_start = datetime.strptime(tr_years[i], '%Y%m%d') #get year start\n",
    "    year_data = tr[i] #get data for that year\n",
    "    year_data=pd.to_datetime(year_data[:,0], format='%Y%m%d') #convert to datetime objects \n",
    "    year_data.to_numpy() \n",
    "    year_data = year_data - tr_year_start #compute days since start of year\n",
    "    year_data = year_data / np.timedelta64(1, 'D') #convert timedeltas to integers\n",
    "    year_data=year_data.to_numpy() #convert dataframe to array\n",
    "    train_dates.append(year_data) #add to train set \n",
    "    \n",
    "   \n",
    "#convert minutes\n",
    "for i in range(len(tr_years)):\n",
    "    \n",
    "    #if statements because some years have different number of data\n",
    "    if tr[i][:,1].shape == (105120,):\n",
    "        #creates to arrays of hours and minutes each\n",
    "        h_n_m = np.divmod(tr[i][:,1],np.full((105120,), 100)) \n",
    "        #add hour and minute arrays to get minutes since day start\n",
    "        train_minutes.append((h_n_m[0]*60)+h_n_m[1]) #add to train_set\n",
    "        h_n_m =0 #reset for next iter\n",
    "        \n",
    "    else: \n",
    "        h_n_m = np.divmod(tr[i][:,1],np.full((105408,), 100))\n",
    "        train_minutes.append((h_n_m[0]*60)+h_n_m[1])\n",
    "        h_n_m =0\n",
    "        \n",
    "#merge everything\n",
    "train_set = []\n",
    "for i in range(len(tr_years)):\n",
    "    train_set.append(np.column_stack((train_minutes[i], train_dates[i],tr[i][:,2])))\n",
    "    \n",
    "train_set #this is a list of 5 np arrays of minutes, dates, and temp columns each\n",
    "#turn this into a big numpy array \n",
    "train = np.vstack((train_set[0],train_set[1],train_set[2],train_set[3],train_set[4]))\n",
    "np.random.shuffle(train)\n",
    "print('training set shape ==', train.shape)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d4e49",
   "metadata": {},
   "source": [
    "### Part 1 - Applying Radial Basis Functions \n",
    "Cover each input dimension with a list of radial basis functions. This turns the pair of inputs into a much richer representation, mapping (d,t) into (Φ₁(d), Φ₂(t)). Experiment with different numbers of radial basis functions and different widths of the radial basis function in different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "x_train = train[:,[0,1]].reshape(-1,1)\n",
    "y_train = train[:,2].reshape(-1, 1)\n",
    "x_test = test[:,[0,1]]\n",
    "y_test = test[:,2]\n",
    "\n",
    "sigma = 0.1\n",
    "alp = 0.0001\n",
    "rbf = rbf_kernel(y_train, x_train, gamma=1.0/sigma)\n",
    "#regression = Ridge(alpha=alp, fit_intercept=False)\n",
    "#regression.fit(rbf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X = train[:,[0,1]]\n",
    "#rbfi = RBF(x,y,)\n",
    "\n",
    "\n",
    "from scipy.interpolate import Rbf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "x, y, z, d = rng.random((4, 50))\n",
    "print(x.shape)\n",
    "rbfi = Rbf(x, y, z, d)  # radial basis function interpolator instance\n",
    "xi = yi = zi = np.linspace(0, 1, 20)\n",
    "di = rbfi(xi, yi, zi)   # interpolated values\n",
    "di.shape\n",
    "di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centroids = 100\n",
    "X = train[:,[0,1]]\n",
    "\n",
    "def get_centroids(X, n_centroids):\n",
    "    # Find the indeces\n",
    "    idx = np.random.randint(np.size(X, axis=0), size=n_centroids)\n",
    "    # Use indeces to grab rows\n",
    "    return(X[idx, :])\n",
    "\n",
    "\n",
    "def rbf(centroid):\n",
    "    # define a closure\n",
    "    def rbfdot(x):\n",
    "        return(np.exp(-np.sum((x - centroid)**2) / sigmasq))\n",
    "    return(rbfdot)\n",
    "\n",
    "\n",
    "centroids = get_centroids(X, n_centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99784778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "n = 1000\n",
    "d = 2\n",
    "m = 100\n",
    "reg_coef = 0.1\n",
    "b = 2\n",
    "n_centroids = 100\n",
    "\n",
    "\n",
    "# Explanatory variables are uniformly distributed\n",
    "#X = np.random.uniform(-4, 4, size=(n, d))\n",
    "X = train[:,[0,1]]\n",
    "\n",
    "# Response is a multivariate normal \n",
    "target_normal = multivariate_normal(mean=np.random.normal(size=d), cov=np.eye(d))\n",
    "y = target_normal.pdf(X)\n",
    "\n",
    "Xhat = np.random.uniform(-4, 4, size=(n, d))\n",
    "yactual = target_normal.pdf(Xhat)\n",
    "\n",
    "def compute_sigmasq(X): \n",
    "    xcross = np.dot(X, X.T)\n",
    "    xnorms = np.repeat(np.diag(np.dot(X, X.T)).reshape(1, -1), np.size(X, axis=0), axis=0)\n",
    "    return(np.median(xnorms - 2*xcross + xnorms.T))\n",
    "# Find sigmasquared for the rbf\n",
    "sigmasq = compute_sigmasq(X)\n",
    "\n",
    "def get_centroids(X, n_centroids):\n",
    "    # Find the indeces\n",
    "    idx = np.random.randint(np.size(X, axis=0), size=n_centroids)\n",
    "    # Use indeces to grab rows\n",
    "    return(X[idx, :])\n",
    "\n",
    "centroids = get_centroids(X, n_centroids)\n",
    "\n",
    "\n",
    "## After this line my implementations #####\n",
    "\n",
    "def cal_distance(x,cent_i):\n",
    "    ## calculate the distance between the centroid and the other elements\n",
    "    return(np.sum((x - cent_i)**2))\n",
    "\n",
    "\n",
    "phi_train = np.ones((n,1))\n",
    "for i in range(n_centroids):\n",
    "    cent_i  = centroids[i,:] # get the ith centroid\n",
    "    dist_i = np.apply_along_axis(cal_distance , 1, X,cent_i) ## distance matrix for i th centroid\n",
    "    phi_i = np.exp(-dist_i/sigmasq) ## \n",
    "    phi_i = np.reshape(phi_i,(n,1)) # dummy rehsape to (n,1)  \n",
    "    phi_train = np.hstack((phi_train,phi_i)) # horizontally stack the matrix\n",
    "\n",
    "inv_phi = np.linalg.pinv(phi_train) # get the psudoinverse for regular linear regression\n",
    "\n",
    "#linear regression coefficiet\n",
    "coefs = np.matmul(inv_phi,y) ## compute the weights\n",
    "coefs = np.reshape(coefs,(n_centroids+1,1))  ## dummy reshape\n",
    "\n",
    "## create phi test\n",
    "phi_test = np.ones((n,1))\n",
    "for i in range(n_centroids):\n",
    "    cent_i  = centroids[i,:] # get the ith centroid\n",
    "    dist_i = np.apply_along_axis(cal_distance , 1, Xhat,cent_i) ## distance matrix for i th centroid\n",
    "    phi_i = np.exp(-dist_i/sigmasq)\n",
    "    phi_i = np.reshape(phi_i,(n,1)) # dummy rehsape \n",
    "    phi_test = np.hstack((phi_test,phi_i))\n",
    "\n",
    "\n",
    "y_predict = np.matmul(phi_test,coefs)\n",
    "\n",
    "plt.plot(yactual,y_predict,'*')\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.title('n_centroid = ' +  str(n_centroids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f82b68",
   "metadata": {},
   "source": [
    "### Part 2 - Build Linear Parameter Model \n",
    "Using this new representation, build a linear parameter model that captures both seasonal variations and daily variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12823742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94d7eb6e",
   "metadata": {},
   "source": [
    "## Part 3 - Visualization\n",
    "- Create two plots, one showing the time-of-day contribution, and one showing the time-of-year contribution.\n",
    "- (Optional) Make a 3D plot showing temperature as a function of (day, time). Make sure to label your axes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c587769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea517c56",
   "metadata": {},
   "source": [
    "## Part 4 - Evaluation \n",
    "Using R², quantify how your model performs on the testing data if you:\n",
    "- Train with just the daily component of the model\n",
    "- Train with just the yearly component of the model\n",
    "- Train with the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a8239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### PCW ### \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics.pairwise import rbf_kernel #get radial basis function kernel \n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "years = range(2011, 2017)\n",
    "files = ['CRNS0101-05-%d-CA_Yosemite_Village_12_W.txt' % y for y in years]\n",
    "\n",
    "\n",
    "usecols = [1, 2, 8] #[WBANNO (station number), UTC Date, Latitude]\n",
    "\n",
    "data = [np.loadtxt(f, usecols=usecols) for f in files] #load data with relevant columns\n",
    "#vstack() function is used to stack the sequence of input arrays vertically to make a single array. \n",
    "data = np.vstack(data) \n",
    "\n",
    "print(data)\n",
    "\n",
    "# Map data from HHmm to an integer\n",
    "data[:, 1] = np.floor_divide(data[:, 1], 100) * 60 + np.mod(data[:, 1], 100)\n",
    "valid = data[:, 2] > -1000 \n",
    "\n",
    "x_train = data[valid, 1].reshape(-1, 1) #utc time in minutes \n",
    "y_train = data[valid, 2] #latitude\n",
    "\n",
    "import random \n",
    "sigma = 0.1\n",
    "alp = 0.0001\n",
    "\n",
    "number_of_rows = x_train.shape[0]\n",
    "random_indices = np.random.choice(number_of_rows, size=1000, replace=True)\n",
    "w = np.random.sample(size =1000)\n",
    "\n",
    "x_train = x_train.reshape(-1,1)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "x_train = x_train[random_indices, :]\n",
    "y_train = y_train[random_indices, :]\n",
    "\n",
    "print(max(x_train), max(y_train))\n",
    "print(min(x_train), min(y_train))\n",
    "print(len(x_train),len(y_train))\n",
    "\n",
    "rbf = rbf_kernel(y_train, x_train, gamma=1.0/sigma)\n",
    "regression = Ridge(alpha=alp, fit_intercept=False)\n",
    "regression.fit(rbf, y_train)\n",
    "\n",
    "print(\"Score on training data = \", regression.score(rbf, y_train))\n",
    "all_rbf = np.linspace(-3.0, 5.0, 1000).reshape(-1, 1)\n",
    "\n",
    "# New representation:\n",
    "expanded_rbf = rbf_kernel(all_rbf, y_train, gamma=1 / sig)\n",
    "all_y = regression.predict(expanded_rbf)\n",
    "\n",
    "print(\"all_x.shape\", all_rbf.shape)\n",
    "print(\"expanded_x.shape\", expanded_rbf.shape)\n",
    "print(\"all_y.shape\", all_y.shape)\n",
    "\n",
    "# Show that the predictions tend to zero far away from inputs\n",
    "plt.figure()\n",
    "plt.plot(all_rbf, all_y)\n",
    "#plt.scatter(x_train, weights)\n",
    "\n",
    "# Zoom in and see how well predictions fit the data\n",
    "zoom_ind = (all_rbf > x_train.min()) & (all_rbf < x_train.max())\n",
    "plt.figure()\n",
    "print(len(zoom_ind))\n",
    "\n",
    "#plt.plot(all_rbf[zoom_ind], all_y[zoom_ind])\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
